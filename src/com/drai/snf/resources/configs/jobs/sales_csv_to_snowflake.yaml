version: "1.2"

metadata:
  name: "csv_multi_sales_ingestion"
  owner: "data-eng-team"
  description: "Load multiple CSV datasets from local system into Snowflake"

datasets:
  - name: "customer_data"
    source:
      type: "s3"
    target:
      type: "snowflake"
      database: "DRAI_INGESTION_DATABASE"
      schema: "STG_SCHEMA"
      table: "CUSTOMER_DATA_PARQUET"
      stage:
        name: "DRAI_S3_STG_INBOUND" 
        path: "customer_data_parquet"
        file_format: "DRAI_PARQUET_FORMAT"
        validate: true
      tbl_schema:
        SALE_ID: "NUMBER"
        PRODUCT_ID: "NUMBER"
        QUANTITY: "NUMBER"
        SALE_AMOUNT: "FLOAT"
      ingestion:
        method: "copy_into"
        copy_options:
          load_mode: "continue"   # abort | continue | truncate | recreate
          load_strategy: "truncate_and_load"       # options: truncate_and_load | drop_and_create | select_matched | default_missing
          validation_mode: false  # true → only validate, don’t load
          stage_path: "@DRAI_S3_STG_INBOUND/customer_data_parquet"
          file_format: "DRAI_PARQUET_FORMAT"

  
# Root-level configs — apply globally
audit:
  enabled: true
  table: "INGESTION_AUDIT_LOG"
  schema: "AUDIT"

error_handling:
  enable_error_table: true
  error_table: "ERROR_LOG"
  error_schema: "ERROR_SCHEMA"  

logging:
  level: "INFO"
  log_path: "src/com/dataready_ai/snf/resources/logs/"
  file_pattern: "{source_type}_{dataset_name}.log"

connections:
  source: "sales_csv"
  target: "sales_snowflake"